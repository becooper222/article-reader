Development and validation of the provider documentation summarization quality instrument for large language models. By Emma Croxford, Yanjun Gao, Nicholas Pellegrino, Karen Wong, Graham Wills, Elliot First, Miranda Schnier, Kyle Burton, Cris Ebby, Jillian Gorski, Matthew Kalscheur, Samy Khalil, Marie Pisani, Tyler Rubeor, Peter Stetson, Frank Liao, Cherodeep Goswami, Brian Patterson, and Majid Afshar.

Abstract.

As large language models are integrated into electronic health record workflows, validated instruments are essential to evaluate their performance. This study developed and validated the Provider Documentation Summarization Quality Instrument, or PDSQI-9, to evaluate LLM-generated clinical summaries. Multi-document summaries were generated from real-world EHR data using several LLMs, and seven physician raters evaluated 779 of them. The PDSQI-9 demonstrated strong internal consistency, with a Cronbach's alpha of 0.879, and high inter-rater reliability, with an Intraclass Correlation Coefficient of 0.867. Factor analysis identified a four-factor model representing organization, clarity, accuracy, and utility. The instrument also successfully distinguished between high- and low-quality summaries. The PDSQI-9 demonstrates robust validity, supporting its use in clinical practice to evaluate LLM-generated summaries and facilitate safer, more effective integration into healthcare workflows.

Introduction.

The volume of notes in the electronic health record, or EHR, has increased dramatically in the past decade, exacerbating the burden on providers. This "note bloat" makes it increasingly difficult to find actionable information. Multi-document summarization using large language models, or LLMs, has emerged as a promising way to reduce this cognitive burden. However, these new capabilities also introduce challenges. LLMs may not fully process all the text in long documents, leading to chronological errors or missed details, which is a major concern when patient safety is a priority. Despite their potential, the rapid advancement of LLMs has outpaced the development of robust tools to evaluate their output quality. Existing instruments for provider documentation were not designed to address the unique challenges of LLM-generated text, such as hallucinations, omissions, and factual accuracy. To address these gaps, this study introduces the PDSQI-9, an instrument specifically designed to evaluate LLM-generated summaries, developed and validated on real-world EHR data.

Methods.

The study used a corpus of notes from inpatient and outpatient encounters at the University of Wisconsin Hospitals and Clinics. The evaluation was designed from the perspective of a provider seeing a patient for the first time who needs a summary of the patient’s prior encounters. The final dataset included 200 unique patients, with their encounter notes stratified across 11 different specialties. The instrument was developed using a semi-Delphi methodology, an iterative, consensus-driven approach involving a panel of nine experts with diverse expertise, including physicians, software developers, and data scientists. The team began with an existing tool, the Physician Documentation Quality Instrument, as a benchmark. They then adapted it for LLMs, removing two attributes and adding two new ones: one to check for stigmatizing language, and another to verify the inclusion of citations that link facts in the summary back to the original notes. The final instrument includes nine attributes: Cited, Accurate, Thorough, Useful, Organized, Comprehensible, Succinct, Synthesized, and Stigmatizing. The team placed a particular focus on defining how the 'Accurate' attribute relates to hallucinations, and how the 'Thorough' attribute relates to omissions, providing clear definitions for both. For example, a pertinent omission was defined as information that is essential and would have adverse effects on patient care decisions if left out.

To generate summaries of varying quality, researchers used several different LLMs, including GPT-4o, Mixtral 8x7B, and Llama 3-8B, all within a secure, HIPAA-compliant environment. Different prompting strategies were used to create both high- and low-quality summaries. For example, to generate low-quality summaries, prompts included "Anti-Rules" that encouraged intentional errors like hallucinations and omissions. Seven physician raters were recruited to evaluate the summaries. They underwent standardized training to ensure consistent application of the instrument. The PDSQI-9 was then rigorously validated through multiple metrics to assess its reliability and validity, including inter-rater reliability using the Intraclass Correlation Coefficient, internal consistency using Cronbach's alpha, and a confirmatory factor analysis.

Results.

Seven physician raters evaluated a total of 779 summaries, and no significant difference in scoring was observed between junior and senior physicians. The original provider notes had a median word count of nearly 3,000 words, while the LLM-generated summaries had a median of about 330 words. The study found that as the length of the source notes increased, the quality of the generated summaries was rated lower in the attributes of Organized, Succinct, and Thorough. Disagreement among raters also increased for longer notes. As intended, the scores from the raters spanned the entire range of the PDSQI-9 for nearly all attributes. The instrument also showed strong discriminant validity, meaning it could reliably distinguish between high- and low-quality summaries.

Reliability metrics for the instrument were strong. The overall intraclass correlation coefficient was 0.867, and Cronbach's alpha was 0.879, indicating good internal consistency. A factor analysis identified a four-factor model that explained 58% of the variance in the data. These four factors represent key dimensions of quality: organization, clarity, accuracy, and utility.

Discussion.

This study introduces the PDSQI-9 as a novel and rigorously validated instrument designed to assess the quality of LLM-generated summaries of clinical documentation. The tool showed strong inter-rater reliability and internal consistency, with consistent performance across evaluators with varying levels of clinical experience. The four-factor model aligns with theoretical constructs relevant to evaluating LLM-generated clinical summaries, validating the instrument's use for this purpose. The semi-Delphi development process ensured the instrument’s attributes are clinically relevant and address critical issues unique to LLMs, such as hallucinations, omissions, and stigmatizing language. This focus reinforces safer applications of LLMs in clinical practice. The evaluation also revealed that as input text gets longer, summary quality tends to decline, highlighting the need for careful consideration when deploying these models. While this human evaluation framework is robust, an important next step is to establish clinical cut-off points to classify summaries as "usable" or "unusable" for patient care. Future work will also focus on the potential for automation to reduce the significant human labor costs associated with this kind of reliable evaluation.

Limitations.

The results of this study should be considered in light of some limitations. The study focuses on a single task—multi-document summarization—and uses a dataset from a single health system. The applicability of these results to other natural language generation tasks or other health systems would require additional testing for external validation.